{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569ca62d-f47a-4e41-9f44-e8682cfcaf92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the logistic regression model: 0.99\n",
      "Model coefficients (θ): [[-3.36825847 -1.88751582 -2.30754002 -0.0880308 ]]\n",
      "Model intercept (θ0): [3.73908395]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "column_names = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "data = pd.read_csv('./code/data/banknote.txt', names=column_names)\n",
    "\n",
    "x = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "logistic_model = LogisticRegression() \n",
    "\n",
    "logistic_model.fit(x, y)\n",
    "\n",
    "y_pred = logistic_model.predict(x)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f'Accuracy of the logistic regression model: {accuracy:.2f}')\n",
    "\n",
    "print('Model coefficients (θ):', logistic_model.coef_) # Theta\n",
    "print('Model intercept (θ0):', logistic_model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634c8162-e3d9-4cff-a706-b779c012d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy1(x,theta_lgt,k):\n",
    "    eta_x = (1 + np.exp(-np.dot(theta_lgt.T, x)))**(-1/k)\n",
    "    return eta_x\n",
    "\n",
    "def strategy2(x,theta_lgt,k):\n",
    "    eta_x = 1 - strategy1(x,theta_lgt,k)\n",
    "    return eta_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505c255c-d663-423a-8420-cfc72ba12e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = x.values\n",
    "theta_lgt = logistic_model.coef_[0]\n",
    "k = 10\n",
    "pi = 0.3\n",
    "data['eta'] = [strategy1(x_i, theta_lgt, k) for x_i in x_values]\n",
    "\n",
    "data_with_pu_label = data.sort_values(by='eta', ascending=False)\n",
    "\n",
    "P_num = sum(data_with_pu_label['class']==0)\n",
    "\n",
    "# 选择前 (1-pi)*正例 的样本作为正例\n",
    "data_with_pu_label['pu_label'] = 0  # 创建一个新列以存储 PU 标签\n",
    "data_with_pu_label.iloc[:int((1-pi)*P_num), -1] = 1  # 前设置为观测到的正例 即 s = 1\n",
    "\n",
    "data_group = data_with_pu_label.groupby('pu_label')\n",
    "S_P = data_group.get_group(1)\n",
    "S_U = data_group.get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b33b097-0023-4dad-9711-e14c504add92",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_P = S_P.iloc[:, :4]\n",
    "S_P['y1'] = 1\n",
    "S_P['y0'] = 0\n",
    "S_P['si'] = 1\n",
    "S_P['yi'] = 1\n",
    "\n",
    "S_U = S_U.iloc[:, :4]\n",
    "S_U['y1'] = 0 # 这里S_U在进入EM算法前应该根据已初始化的theta值，计算出widetilde{P(y_i)}的值\n",
    "S_U['y0'] = 0\n",
    "S_U['si'] = 0\n",
    "S_U['yi'] = 3 # 用3 表示 Unknown\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xi_SU = torch.tensor(S_U.drop(['y1','y0','si','yi'], axis=1).values, dtype=torch.float32)\n",
    "y1_SU = torch.tensor(S_U['y1'].values, dtype=torch.int64)\n",
    "y0_SU = torch.tensor(S_U['y0'].values, dtype=torch.int64)\n",
    "si_SU = torch.tensor(S_U['si'].values, dtype=torch.int64)\n",
    "yi_SU = torch.tensor(S_U['yi'].values, dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "S_U = [[xi_SU[i], y1_SU[i],y0_SU[i],si_SU[i],yi_SU[i]] for i in range(len(S_U))]\n",
    "\n",
    "xi_SP = torch.tensor(S_P.drop(['y1','y0','si','yi'], axis=1).values, dtype=torch.float32)\n",
    "y1_SP = torch.tensor(S_P['y1'].values, dtype=torch.int64)\n",
    "y0_SP = torch.tensor(S_P['y0'].values, dtype=torch.int64)\n",
    "si_SP = torch.tensor(S_P['si'].values, dtype=torch.int64)\n",
    "yi_SP = torch.tensor(S_P['yi'].values, dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "S_P = [[xi_SP[i], y1_SP[i],y0_SP[i],si_SP[i],yi_SP[i]] for i in range(len(S_P))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0582db7-e96d-4f4a-8d56-52f7dc28547e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6236fa2-55df-4c8c-a530-0d5df8727f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init theta1 theta2\n",
    "\n",
    "x = data_with_pu_label.iloc[:, :4]\n",
    "y = data_with_pu_label['pu_label']\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x, y)\n",
    "\n",
    "theta1 = torch.tensor(logistic_model.coef_[0],dtype=torch.float32)\n",
    "theta2 = torch.zeros(4,dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e765907-70b9-4d3d-be02-b20a1a73455c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.1456, -2.5409, -3.2537, -0.0525]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1,theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2c7d08-2142-4c7c-a9f1-75728fb080b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cqut1\\AppData\\Local\\Temp\\ipykernel_48864\\3329105554.py:18: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  return 1.0 / (1.0 + torch.exp(-torch.matmul(theta.T, x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], theta1: tensor([-5.1556, -2.5509, -3.2437, -0.0625], requires_grad=True), theta2: tensor([0.0100, 0.0100, 0.0100, 0.0100], requires_grad=True)\n",
      "Epoch [2/100], theta1: tensor([-5.1654, -2.5603, -3.2344, -0.0725], requires_grad=True), theta2: tensor([0.0200, 0.0200, 0.0200, 0.0200], requires_grad=True)\n",
      "Epoch [3/100], theta1: tensor([-5.1748, -2.5681, -3.2270, -0.0824], requires_grad=True), theta2: tensor([0.0300, 0.0300, 0.0300, 0.0300], requires_grad=True)\n",
      "Epoch [4/100], theta1: tensor([-5.1836, -2.5732, -3.2228, -0.0923], requires_grad=True), theta2: tensor([0.0400, 0.0400, 0.0400, 0.0400], requires_grad=True)\n",
      "Epoch [5/100], theta1: tensor([-5.1916, -2.5754, -3.2219, -0.1022], requires_grad=True), theta2: tensor([0.0500, 0.0500, 0.0500, 0.0500], requires_grad=True)\n",
      "Epoch [6/100], theta1: tensor([-5.1990, -2.5752, -3.2234, -0.1119], requires_grad=True), theta2: tensor([0.0600, 0.0600, 0.0600, 0.0600], requires_grad=True)\n",
      "Epoch [7/100], theta1: tensor([-5.2059, -2.5734, -3.2267, -0.1214], requires_grad=True), theta2: tensor([0.0700, 0.0700, 0.0700, 0.0700], requires_grad=True)\n",
      "Epoch [8/100], theta1: tensor([-5.2124, -2.5705, -3.2310, -0.1308], requires_grad=True), theta2: tensor([0.0800, 0.0800, 0.0800, 0.0800], requires_grad=True)\n",
      "Epoch [9/100], theta1: tensor([-5.2188, -2.5671, -3.2359, -0.1398], requires_grad=True), theta2: tensor([0.0900, 0.0900, 0.0900, 0.0900], requires_grad=True)\n",
      "Epoch [10/100], theta1: tensor([-5.2252, -2.5637, -3.2410, -0.1485], requires_grad=True), theta2: tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "Epoch [11/100], theta1: tensor([-5.2318, -2.5607, -3.2460, -0.1568], requires_grad=True), theta2: tensor([0.1099, 0.1099, 0.1099, 0.1099], requires_grad=True)\n",
      "Epoch [12/100], theta1: tensor([-5.2388, -2.5585, -3.2505, -0.1647], requires_grad=True), theta2: tensor([0.1199, 0.1199, 0.1199, 0.1199], requires_grad=True)\n",
      "Epoch [13/100], theta1: tensor([-5.2461, -2.5573, -3.2543, -0.1719], requires_grad=True), theta2: tensor([0.1299, 0.1299, 0.1299, 0.1299], requires_grad=True)\n",
      "Epoch [14/100], theta1: tensor([-5.2537, -2.5573, -3.2572, -0.1787], requires_grad=True), theta2: tensor([0.1398, 0.1398, 0.1398, 0.1398], requires_grad=True)\n",
      "Epoch [15/100], theta1: tensor([-5.2616, -2.5583, -3.2594, -0.1848], requires_grad=True), theta2: tensor([0.1498, 0.1498, 0.1498, 0.1498], requires_grad=True)\n",
      "Epoch [16/100], theta1: tensor([-5.2697, -2.5603, -3.2608, -0.1902], requires_grad=True), theta2: tensor([0.1597, 0.1597, 0.1597, 0.1597], requires_grad=True)\n",
      "Epoch [17/100], theta1: tensor([-5.2780, -2.5630, -3.2617, -0.1950], requires_grad=True), theta2: tensor([0.1697, 0.1697, 0.1697, 0.1697], requires_grad=True)\n",
      "Epoch [18/100], theta1: tensor([-5.2863, -2.5662, -3.2624, -0.1992], requires_grad=True), theta2: tensor([0.1796, 0.1796, 0.1796, 0.1796], requires_grad=True)\n",
      "Epoch [19/100], theta1: tensor([-5.2946, -2.5697, -3.2629, -0.2028], requires_grad=True), theta2: tensor([0.1895, 0.1895, 0.1895, 0.1895], requires_grad=True)\n",
      "Epoch [20/100], theta1: tensor([-5.3028, -2.5732, -3.2637, -0.2057], requires_grad=True), theta2: tensor([0.1993, 0.1993, 0.1993, 0.1993], requires_grad=True)\n",
      "Epoch [21/100], theta1: tensor([-5.3107, -2.5765, -3.2649, -0.2081], requires_grad=True), theta2: tensor([0.2092, 0.2092, 0.2092, 0.2092], requires_grad=True)\n",
      "Epoch [22/100], theta1: tensor([-5.3185, -2.5796, -3.2666, -0.2098], requires_grad=True), theta2: tensor([0.2190, 0.2190, 0.2190, 0.2190], requires_grad=True)\n",
      "Epoch [23/100], theta1: tensor([-5.3259, -2.5822, -3.2689, -0.2110], requires_grad=True), theta2: tensor([0.2288, 0.2288, 0.2288, 0.2288], requires_grad=True)\n",
      "Epoch [24/100], theta1: tensor([-5.3331, -2.5844, -3.2718, -0.2117], requires_grad=True), theta2: tensor([0.2386, 0.2386, 0.2386, 0.2386], requires_grad=True)\n",
      "Epoch [25/100], theta1: tensor([-5.3399, -2.5862, -3.2753, -0.2119], requires_grad=True), theta2: tensor([0.2484, 0.2484, 0.2484, 0.2484], requires_grad=True)\n",
      "Epoch [26/100], theta1: tensor([-5.3465, -2.5877, -3.2792, -0.2115], requires_grad=True), theta2: tensor([0.2582, 0.2582, 0.2582, 0.2582], requires_grad=True)\n",
      "Epoch [27/100], theta1: tensor([-5.3529, -2.5889, -3.2836, -0.2107], requires_grad=True), theta2: tensor([0.2679, 0.2679, 0.2679, 0.2679], requires_grad=True)\n",
      "Epoch [28/100], theta1: tensor([-5.3592, -2.5900, -3.2881, -0.2095], requires_grad=True), theta2: tensor([0.2776, 0.2776, 0.2776, 0.2776], requires_grad=True)\n",
      "Epoch [29/100], theta1: tensor([-5.3654, -2.5912, -3.2928, -0.2078], requires_grad=True), theta2: tensor([0.2872, 0.2872, 0.2872, 0.2872], requires_grad=True)\n",
      "Epoch [30/100], theta1: tensor([-5.3716, -2.5925, -3.2975, -0.2057], requires_grad=True), theta2: tensor([0.2968, 0.2968, 0.2968, 0.2968], requires_grad=True)\n",
      "Epoch [31/100], theta1: tensor([-5.3778, -2.5940, -3.3021, -0.2032], requires_grad=True), theta2: tensor([0.3064, 0.3064, 0.3064, 0.3064], requires_grad=True)\n",
      "Epoch [32/100], theta1: tensor([-5.3841, -2.5959, -3.3065, -0.2004], requires_grad=True), theta2: tensor([0.3160, 0.3160, 0.3160, 0.3160], requires_grad=True)\n",
      "Epoch [33/100], theta1: tensor([-5.3905, -2.5983, -3.3106, -0.1973], requires_grad=True), theta2: tensor([0.3255, 0.3255, 0.3255, 0.3255], requires_grad=True)\n",
      "Epoch [34/100], theta1: tensor([-5.3970, -2.6010, -3.3145, -0.1938], requires_grad=True), theta2: tensor([0.3350, 0.3350, 0.3350, 0.3350], requires_grad=True)\n",
      "Epoch [35/100], theta1: tensor([-5.4036, -2.6041, -3.3181, -0.1901], requires_grad=True), theta2: tensor([0.3445, 0.3445, 0.3445, 0.3445], requires_grad=True)\n",
      "Epoch [36/100], theta1: tensor([-5.4103, -2.6075, -3.3215, -0.1862], requires_grad=True), theta2: tensor([0.3539, 0.3539, 0.3539, 0.3539], requires_grad=True)\n",
      "Epoch [37/100], theta1: tensor([-5.4170, -2.6111, -3.3248, -0.1820], requires_grad=True), theta2: tensor([0.3632, 0.3632, 0.3632, 0.3632], requires_grad=True)\n",
      "Epoch [38/100], theta1: tensor([-5.4237, -2.6149, -3.3280, -0.1777], requires_grad=True), theta2: tensor([0.3726, 0.3726, 0.3726, 0.3726], requires_grad=True)\n",
      "Epoch [39/100], theta1: tensor([-5.4305, -2.6187, -3.3313, -0.1733], requires_grad=True), theta2: tensor([0.3819, 0.3819, 0.3819, 0.3819], requires_grad=True)\n",
      "Epoch [40/100], theta1: tensor([-5.4372, -2.6225, -3.3346, -0.1687], requires_grad=True), theta2: tensor([0.3911, 0.3911, 0.3911, 0.3911], requires_grad=True)\n",
      "Epoch [41/100], theta1: tensor([-5.4438, -2.6263, -3.3381, -0.1640], requires_grad=True), theta2: tensor([0.4004, 0.4004, 0.4004, 0.4004], requires_grad=True)\n",
      "Epoch [42/100], theta1: tensor([-5.4503, -2.6298, -3.3419, -0.1593], requires_grad=True), theta2: tensor([0.4095, 0.4095, 0.4095, 0.4095], requires_grad=True)\n",
      "Epoch [43/100], theta1: tensor([-5.4567, -2.6332, -3.3458, -0.1545], requires_grad=True), theta2: tensor([0.4187, 0.4187, 0.4187, 0.4187], requires_grad=True)\n",
      "Epoch [44/100], theta1: tensor([-5.4631, -2.6364, -3.3500, -0.1497], requires_grad=True), theta2: tensor([0.4278, 0.4278, 0.4278, 0.4278], requires_grad=True)\n",
      "Epoch [45/100], theta1: tensor([-5.4693, -2.6394, -3.3543, -0.1449], requires_grad=True), theta2: tensor([0.4368, 0.4368, 0.4368, 0.4368], requires_grad=True)\n",
      "Epoch [46/100], theta1: tensor([-5.4755, -2.6423, -3.3589, -0.1400], requires_grad=True), theta2: tensor([0.4458, 0.4458, 0.4458, 0.4458], requires_grad=True)\n",
      "Epoch [47/100], theta1: tensor([-5.4817, -2.6451, -3.3635, -0.1352], requires_grad=True), theta2: tensor([0.4548, 0.4548, 0.4548, 0.4548], requires_grad=True)\n",
      "Epoch [48/100], theta1: tensor([-5.4879, -2.6479, -3.3681, -0.1303], requires_grad=True), theta2: tensor([0.4637, 0.4637, 0.4637, 0.4637], requires_grad=True)\n",
      "Epoch [49/100], theta1: tensor([-5.4940, -2.6507, -3.3728, -0.1255], requires_grad=True), theta2: tensor([0.4726, 0.4726, 0.4726, 0.4726], requires_grad=True)\n",
      "Epoch [50/100], theta1: tensor([-5.5003, -2.6536, -3.3774, -0.1207], requires_grad=True), theta2: tensor([0.4814, 0.4814, 0.4814, 0.4814], requires_grad=True)\n",
      "Epoch [51/100], theta1: tensor([-5.5066, -2.6566, -3.3819, -0.1160], requires_grad=True), theta2: tensor([0.4902, 0.4902, 0.4902, 0.4902], requires_grad=True)\n",
      "Epoch [52/100], theta1: tensor([-5.5129, -2.6597, -3.3863, -0.1112], requires_grad=True), theta2: tensor([0.4990, 0.4990, 0.4990, 0.4990], requires_grad=True)\n",
      "Epoch [53/100], theta1: tensor([-5.5193, -2.6630, -3.3906, -0.1065], requires_grad=True), theta2: tensor([0.5077, 0.5077, 0.5077, 0.5077], requires_grad=True)\n",
      "Epoch [54/100], theta1: tensor([-5.5258, -2.6664, -3.3948, -0.1019], requires_grad=True), theta2: tensor([0.5164, 0.5164, 0.5164, 0.5164], requires_grad=True)\n",
      "Epoch [55/100], theta1: tensor([-5.5324, -2.6699, -3.3988, -0.0973], requires_grad=True), theta2: tensor([0.5250, 0.5250, 0.5250, 0.5250], requires_grad=True)\n",
      "Epoch [56/100], theta1: tensor([-5.5390, -2.6734, -3.4028, -0.0927], requires_grad=True), theta2: tensor([0.5336, 0.5336, 0.5336, 0.5336], requires_grad=True)\n",
      "Epoch [57/100], theta1: tensor([-5.5457, -2.6770, -3.4068, -0.0883], requires_grad=True), theta2: tensor([0.5421, 0.5421, 0.5421, 0.5421], requires_grad=True)\n",
      "Epoch [58/100], theta1: tensor([-5.5524, -2.6806, -3.4108, -0.0838], requires_grad=True), theta2: tensor([0.5506, 0.5506, 0.5506, 0.5506], requires_grad=True)\n",
      "Epoch [59/100], theta1: tensor([-5.5590, -2.6842, -3.4148, -0.0795], requires_grad=True), theta2: tensor([0.5591, 0.5591, 0.5591, 0.5591], requires_grad=True)\n",
      "Epoch [60/100], theta1: tensor([-5.5657, -2.6877, -3.4188, -0.0752], requires_grad=True), theta2: tensor([0.5675, 0.5675, 0.5675, 0.5675], requires_grad=True)\n",
      "Epoch [61/100], theta1: tensor([-5.5724, -2.6911, -3.4229, -0.0710], requires_grad=True), theta2: tensor([0.5759, 0.5759, 0.5759, 0.5759], requires_grad=True)\n",
      "Epoch [62/100], theta1: tensor([-5.5790, -2.6945, -3.4271, -0.0668], requires_grad=True), theta2: tensor([0.5842, 0.5842, 0.5842, 0.5842], requires_grad=True)\n",
      "Epoch [63/100], theta1: tensor([-5.5856, -2.6977, -3.4314, -0.0627], requires_grad=True), theta2: tensor([0.5925, 0.5925, 0.5925, 0.5925], requires_grad=True)\n",
      "Epoch [64/100], theta1: tensor([-5.5922, -2.7008, -3.4358, -0.0587], requires_grad=True), theta2: tensor([0.6008, 0.6008, 0.6008, 0.6008], requires_grad=True)\n",
      "Epoch [65/100], theta1: tensor([-5.5988, -2.7039, -3.4402, -0.0547], requires_grad=True), theta2: tensor([0.6090, 0.6090, 0.6090, 0.6090], requires_grad=True)\n",
      "Epoch [66/100], theta1: tensor([-5.6054, -2.7070, -3.4446, -0.0508], requires_grad=True), theta2: tensor([0.6172, 0.6172, 0.6172, 0.6172], requires_grad=True)\n",
      "Epoch [67/100], theta1: tensor([-5.6121, -2.7100, -3.4490, -0.0470], requires_grad=True), theta2: tensor([0.6253, 0.6253, 0.6253, 0.6253], requires_grad=True)\n",
      "Epoch [68/100], theta1: tensor([-5.6187, -2.7131, -3.4535, -0.0432], requires_grad=True), theta2: tensor([0.6334, 0.6334, 0.6334, 0.6334], requires_grad=True)\n",
      "Epoch [69/100], theta1: tensor([-5.6254, -2.7161, -3.4579, -0.0395], requires_grad=True), theta2: tensor([0.6415, 0.6415, 0.6415, 0.6415], requires_grad=True)\n",
      "Epoch [70/100], theta1: tensor([-5.6321, -2.7192, -3.4622, -0.0358], requires_grad=True), theta2: tensor([0.6495, 0.6495, 0.6495, 0.6495], requires_grad=True)\n",
      "Epoch [71/100], theta1: tensor([-5.6388, -2.7224, -3.4665, -0.0321], requires_grad=True), theta2: tensor([0.6575, 0.6575, 0.6575, 0.6575], requires_grad=True)\n",
      "Epoch [72/100], theta1: tensor([-5.6456, -2.7255, -3.4707, -0.0285], requires_grad=True), theta2: tensor([0.6655, 0.6655, 0.6655, 0.6655], requires_grad=True)\n",
      "Epoch [73/100], theta1: tensor([-5.6524, -2.7288, -3.4749, -0.0250], requires_grad=True), theta2: tensor([0.6734, 0.6734, 0.6734, 0.6734], requires_grad=True)\n",
      "Epoch [74/100], theta1: tensor([-5.6593, -2.7320, -3.4791, -0.0215], requires_grad=True), theta2: tensor([0.6813, 0.6813, 0.6813, 0.6813], requires_grad=True)\n",
      "Epoch [75/100], theta1: tensor([-5.6661, -2.7353, -3.4832, -0.0180], requires_grad=True), theta2: tensor([0.6892, 0.6892, 0.6892, 0.6892], requires_grad=True)\n",
      "Epoch [76/100], theta1: tensor([-5.6730, -2.7386, -3.4873, -0.0146], requires_grad=True), theta2: tensor([0.6970, 0.6970, 0.6970, 0.6970], requires_grad=True)\n",
      "Epoch [77/100], theta1: tensor([-5.6799, -2.7419, -3.4915, -0.0112], requires_grad=True), theta2: tensor([0.7047, 0.7047, 0.7047, 0.7047], requires_grad=True)\n",
      "Epoch [78/100], theta1: tensor([-5.6868, -2.7451, -3.4956, -0.0078], requires_grad=True), theta2: tensor([0.7125, 0.7125, 0.7125, 0.7125], requires_grad=True)\n",
      "Epoch [79/100], theta1: tensor([-5.6937e+00, -2.7483e+00, -3.4998e+00, -4.5178e-03],\n",
      "       requires_grad=True), theta2: tensor([0.7202, 0.7202, 0.7202, 0.7202], requires_grad=True)\n",
      "Epoch [80/100], theta1: tensor([-5.7005e+00, -2.7515e+00, -3.5040e+00, -1.2289e-03],\n",
      "       requires_grad=True), theta2: tensor([0.7279, 0.7279, 0.7279, 0.7279], requires_grad=True)\n",
      "Epoch [81/100], theta1: tensor([-5.7074e+00, -2.7546e+00, -3.5082e+00,  2.0290e-03],\n",
      "       requires_grad=True), theta2: tensor([0.7355, 0.7355, 0.7355, 0.7355], requires_grad=True)\n",
      "Epoch [82/100], theta1: tensor([-5.7143e+00, -2.7577e+00, -3.5125e+00,  5.2576e-03],\n",
      "       requires_grad=True), theta2: tensor([0.7432, 0.7432, 0.7432, 0.7432], requires_grad=True)\n",
      "Epoch [83/100], theta1: tensor([-5.7211, -2.7608, -3.5167,  0.0085], requires_grad=True), theta2: tensor([0.7507, 0.7507, 0.7507, 0.7507], requires_grad=True)\n",
      "Epoch [84/100], theta1: tensor([-5.7280, -2.7638, -3.5210,  0.0116], requires_grad=True), theta2: tensor([0.7583, 0.7583, 0.7583, 0.7583], requires_grad=True)\n",
      "Epoch [85/100], theta1: tensor([-5.7348, -2.7669, -3.5253,  0.0148], requires_grad=True), theta2: tensor([0.7658, 0.7658, 0.7658, 0.7658], requires_grad=True)\n",
      "Epoch [86/100], theta1: tensor([-5.7417, -2.7699, -3.5296,  0.0179], requires_grad=True), theta2: tensor([0.7733, 0.7733, 0.7733, 0.7733], requires_grad=True)\n",
      "Epoch [87/100], theta1: tensor([-5.7486, -2.7729, -3.5339,  0.0210], requires_grad=True), theta2: tensor([0.7808, 0.7808, 0.7808, 0.7808], requires_grad=True)\n",
      "Epoch [88/100], theta1: tensor([-5.7554, -2.7760, -3.5382,  0.0241], requires_grad=True), theta2: tensor([0.7882, 0.7882, 0.7882, 0.7882], requires_grad=True)\n",
      "Epoch [89/100], theta1: tensor([-5.7623, -2.7791, -3.5425,  0.0272], requires_grad=True), theta2: tensor([0.7956, 0.7956, 0.7956, 0.7956], requires_grad=True)\n",
      "Epoch [90/100], theta1: tensor([-5.7692, -2.7822, -3.5467,  0.0303], requires_grad=True), theta2: tensor([0.8030, 0.8030, 0.8030, 0.8030], requires_grad=True)\n",
      "Epoch [91/100], theta1: tensor([-5.7762, -2.7853, -3.5509,  0.0333], requires_grad=True), theta2: tensor([0.8103, 0.8103, 0.8103, 0.8103], requires_grad=True)\n",
      "Epoch [92/100], theta1: tensor([-5.7831, -2.7884, -3.5551,  0.0363], requires_grad=True), theta2: tensor([0.8177, 0.8177, 0.8177, 0.8177], requires_grad=True)\n",
      "Epoch [93/100], theta1: tensor([-5.7900, -2.7915, -3.5593,  0.0393], requires_grad=True), theta2: tensor([0.8250, 0.8250, 0.8250, 0.8250], requires_grad=True)\n",
      "Epoch [94/100], theta1: tensor([-5.7969, -2.7947, -3.5635,  0.0423], requires_grad=True), theta2: tensor([0.8322, 0.8322, 0.8322, 0.8322], requires_grad=True)\n",
      "Epoch [95/100], theta1: tensor([-5.8039, -2.7978, -3.5677,  0.0453], requires_grad=True), theta2: tensor([0.8395, 0.8395, 0.8395, 0.8395], requires_grad=True)\n",
      "Epoch [96/100], theta1: tensor([-5.8108, -2.8009, -3.5718,  0.0483], requires_grad=True), theta2: tensor([0.8467, 0.8467, 0.8467, 0.8467], requires_grad=True)\n",
      "Epoch [97/100], theta1: tensor([-5.8177, -2.8041, -3.5760,  0.0513], requires_grad=True), theta2: tensor([0.8539, 0.8539, 0.8539, 0.8539], requires_grad=True)\n",
      "Epoch [98/100], theta1: tensor([-5.8247, -2.8072, -3.5802,  0.0542], requires_grad=True), theta2: tensor([0.8610, 0.8610, 0.8610, 0.8610], requires_grad=True)\n",
      "Epoch [99/100], theta1: tensor([-5.8316, -2.8102, -3.5845,  0.0572], requires_grad=True), theta2: tensor([0.8682, 0.8682, 0.8682, 0.8682], requires_grad=True)\n",
      "Epoch [100/100], theta1: tensor([-5.8385, -2.8133, -3.5887,  0.0601], requires_grad=True), theta2: tensor([0.8753, 0.8753, 0.8753, 0.8753], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class PU_EM_Model:\n",
    "    def __init__(self, theta1, theta2, S_U, S_P):\n",
    "        \"\"\"\n",
    "        dataset = [x_i(4D),y_1,y_0,s_i]\n",
    "        \"\"\"\n",
    "\n",
    "        self.S_U = S_U\n",
    "        self.S_P = S_P\n",
    "        self.dataset = S_U + S_P\n",
    "        self.theta1 = theta1.clone().detach().requires_grad_(True)\n",
    "        self.theta2 = theta2.clone().detach().requires_grad_(True)\n",
    "\n",
    "    def _logistic_function(self, theta, x):\n",
    "        # logistic function\n",
    "        return 1.0 / (1.0 + torch.exp(-torch.matmul(theta.T, x)))\n",
    "\n",
    "    def eta(self, x):\n",
    "        # 公式(9)\n",
    "        return self._logistic_function(self.theta2, x)\n",
    "\n",
    "    def h(self, x):\n",
    "        # 公式(8)\n",
    "        return self._logistic_function(self.theta1, x)\n",
    "\n",
    "    def grad_expressions_theta1(self):\n",
    "        grad_theta1 = torch.zeros_like(self.theta1)\n",
    "        for data in self.dataset:\n",
    "            x = data[0]\n",
    "            y_1 = data[1]\n",
    "            y_0 = data[2]\n",
    "\n",
    "            h_x = self.h(x)\n",
    "            grad_theta1 += y_1 * (h_x - 1) * x + y_0 * h_x * x\n",
    "        return grad_theta1\n",
    "\n",
    "    def grad_expressions_theta2(self):\n",
    "        grad_theta2 = torch.zeros_like(self.theta2)\n",
    "        for data in self.dataset:\n",
    "            x = data[0]\n",
    "            y_1 = data[1]\n",
    "            y_0 = data[2]\n",
    "            s = data[3]\n",
    "\n",
    "            if not s.item():\n",
    "                # s 为 0的情况下\n",
    "                grad_theta2 += y_1 * self.eta(x)\n",
    "            else:\n",
    "                # s 为 1的情况下\n",
    "                grad_theta2 += y_1 * (self.eta(x) - 1)\n",
    "\n",
    "        return grad_theta2\n",
    "\n",
    "    def expectation_step(self):\n",
    "        # 在E步中更新隐变量的期望\n",
    "        for i in range(len(self.dataset)):\n",
    "            # 为了可读性\n",
    "            x = self.dataset[i][0]\n",
    "            s = self.dataset[i][3]\n",
    "\n",
    "            if not s.item():\n",
    "                # s 为 0的情况下 ,通过公式(12)对\\cap{P(y)}的值进行更新\n",
    "                p_y1 = (1 - self.eta(x)) * self.h(x)\n",
    "                p_y0 = 1 - self.h(x)\n",
    "                # 更新数据集中的 y_1 和 y_0\n",
    "                self.dataset[i][1] = p_y1\n",
    "                self.dataset[i][2] = p_y0\n",
    "            else:\n",
    "                # s 为 1的情况下\n",
    "                p_y1 = self.eta(x) * self.h(x)\n",
    "                p_y0 = 0\n",
    "                # 更新数据集中的 y_1 和 y_0\n",
    "                self.dataset[i][1] = p_y1  # 1\n",
    "                self.dataset[i][2] = p_y0  # 0\n",
    "\n",
    "    def maximization_step(self, optimizer):\n",
    "        # 在M步中更新参数 theta1 和 theta2\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度表达式\n",
    "        self.theta1.grad = self.grad_expressions_theta1()  # 计算 theta1 的梯度\n",
    "        self.theta2.grad = self.grad_expressions_theta2()  # 计算 theta2 的梯度\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# 定义优化器参数\n",
    "lr = 0.01  # 学习率\n",
    "rho1 = 0.9  # beta1，用于一阶矩估计的衰减率\n",
    "rho2 = 0.999  # beta2，用于二阶矩估计的衰减率\n",
    "num_epochs = 100\n",
    "\n",
    "model = PU_EM_Model(theta1, theta2, S_U, S_P)\n",
    "optimizer = optim.Adam([model.theta1, model.theta2], lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # E步\n",
    "    model.expectation_step()\n",
    "\n",
    "    # M步\n",
    "    model.maximization_step(optimizer)\n",
    "\n",
    "    # 输出当前参数\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], theta1: {model.theta1}, theta2: {model.theta2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5791cb-632e-47b7-ad68-243e7633014a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb7b250-5542-41ea-ab54-9ea468956e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the manually defined logistic regression model: 0.94\n",
      "Manually defined model coefficients (θ): [-5.8385 -2.8133 -3.5887  0.0601]\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "column_names = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "data = pd.read_csv('./code/data/banknote.txt', names=column_names)\n",
    "\n",
    "x = data.drop('class', axis=1).values\n",
    "y = data['class'].values\n",
    "\n",
    "\n",
    "\n",
    "theta = np.array([-5.8385, -2.8133, -3.5887,  0.0601]) \n",
    "theta_0 = 0\n",
    "\n",
    "z = 1/(np.exp(-np.dot(x, theta))+1)\n",
    "\n",
    "y_pred = (z -0.5 > 0).astype(int)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f'Accuracy of the manually defined logistic regression model: {accuracy:.2f}')\n",
    "\n",
    "# 显示手动设置的参数\n",
    "print('Manually defined model coefficients (θ):', theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fec305-3bfc-4297-beff-4b68b592a234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
