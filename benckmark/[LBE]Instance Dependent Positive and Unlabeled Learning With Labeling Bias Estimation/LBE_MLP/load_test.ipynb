{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97155842-4150-4241-933f-3299c7efb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import torch as t\n",
    "def reader_breast():\n",
    "    data = scio.loadmat('datasets/breast.mat')['breast'].toarray()\n",
    "    features, targets = t.Tensor(data[:, :-1]), t.Tensor(data[:, -1])\n",
    "    targets = t.where(targets != 1, t.zeros_like(targets), targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cb136-4b19-473a-bcde-f77a0a18abe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87df8680-313f-4405-b990-90433f3c4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_australian():\n",
    "    data = scio.loadmat('datasets/Australian.mat')\n",
    "    features, targets = data['fea'], data['gnd']\n",
    "    features, targets = t.Tensor(features.toarray()), t.Tensor(targets).squeeze()\n",
    "    targets = t.where(targets != 1, t.zeros_like(targets), targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26024721-2058-4543-b86c-a7a3053ee626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87790d74-6ead-4303-a4b1-23c3712b9f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "03deb6d7-c279-4cba-9198-2e896fca2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from joblib import dump,load\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.datasets import fetch_20newsgroups,fetch_openml\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "os.environ['http_proxy'] = '127.0.0.1:1066'\n",
    "os.environ['https_proxy'] = '127.0.0.1:1066'\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def _preprocess_data(uci_id):\n",
    "        # fetch dataset\n",
    "        dataset = fetch_ucirepo(id=uci_id)\n",
    "        \n",
    "        # data (as pandas dataframes)\n",
    "        X = dataset.data.features\n",
    "        y = dataset.data.targets\n",
    "        \n",
    "        # 检测并删除缺失值\n",
    "        combined = pd.concat([X, y], axis=1)\n",
    "        combined_cleaned = combined.dropna()\n",
    "\n",
    "        # 分离 features 和 targets\n",
    "        X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "        y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "        # 转换为 PyTorch 张量\n",
    "        features = torch.Tensor(X_cleaned.to_numpy())\n",
    "        targets = torch.Tensor(y_cleaned.to_numpy()).flatten()\n",
    "\n",
    "        return features, targets\n",
    "\n",
    "def get_uci_dataset_by_id(uci_id):\n",
    "\n",
    "    match int(uci_id):\n",
    "        case 15:\n",
    "            # Breast\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 4, torch.tensor(0.), torch.tensor(1.)) # 4为恶性 2为良性 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 143:\n",
    "            # Australian\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 267:\n",
    "            # banknote\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 171:\n",
    "            # Madelon\n",
    "            pass\n",
    "        case 327:\n",
    "            # Phishing\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 105:\n",
    "            # Vote\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        # case 73:\n",
    "        #     # fetch dataset\n",
    "        #     dataset = fetch_ucirepo(id=uci_id)\n",
    "            \n",
    "        #     # data (as pandas dataframes)\n",
    "        #     X = dataset.data.features\n",
    "        #     y = dataset.data.targets\n",
    "        #     # 检测并删除缺失值\n",
    "        #     combined = pd.concat([X, y], axis=1)\n",
    "        #     combined_cleaned = combined.dropna()\n",
    "    \n",
    "        #     # 分离 features 和 targets\n",
    "        #     X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "        #     y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "        #     # print(X_cleaned)\n",
    "        #     from sklearn.preprocessing import LabelEncoder\n",
    "        #     label_encoder_X = LabelEncoder()\n",
    "        #     X_cleaned = X_cleaned.apply(lambda col: label_encoder_X.fit_transform(col))\n",
    "            \n",
    "        #     label_encoder_y = LabelEncoder()\n",
    "        #     y_cleaned = label_encoder_y.fit_transform(y_cleaned)\n",
    "    \n",
    "        #     # 转换为 PyTorch 张量\n",
    "        #     features = torch.Tensor(X_cleaned.to_numpy())\n",
    "        #     targets = torch.Tensor(y_cleaned).flatten()\n",
    "        #     return features, targets\n",
    "\n",
    "        case 73:\n",
    "            # fetch dataset\n",
    "            dataset = fetch_ucirepo(id=uci_id)\n",
    "            \n",
    "            # data (as pandas dataframes)\n",
    "            X = dataset.data.features\n",
    "            y = dataset.data.targets\n",
    "            # 检测并删除缺失值\n",
    "            combined = pd.concat([X, y], axis=1)\n",
    "            combined_cleaned = combined.dropna()\n",
    "    \n",
    "            # 分离 features 和 targets\n",
    "            X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "            y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "            X_vectorized = pd.get_dummies(X_cleaned)\n",
    "            \n",
    "            # 如果 y 是分类变量，也需要转换\n",
    "            # 比如如果 y 也是 'p' 和 'e' 两种类别\n",
    "            y_vectorized = pd.get_dummies(y_cleaned)\n",
    "    \n",
    "            # 转换为 PyTorch 张量\n",
    "            features = torch.Tensor(X_vectorized.to_numpy())\n",
    "            targets = torch.Tensor(y_vectorized.to_numpy())[:,1]\n",
    "            return features, targets\n",
    "        \n",
    "    \n",
    "        case _:\n",
    "            # 所有不需要特殊处理的都可以直接转换\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 改为1,0 \n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "\n",
    "def get_news_dataset_by_categories():\n",
    "    \n",
    "    pos=\"comp\"\n",
    "    neg=\"rec\"\n",
    "    categories = [\n",
    "    #  'alt.atheism',\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "    #  'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "    #  'sci.crypt',\n",
    "    #  'sci.electronics',\n",
    "    #  'sci.med',\n",
    "    #  'sci.space',\n",
    "    #  'soc.religion.christian',\n",
    "    #  'talk.politics.guns',\n",
    "    #  'talk.politics.mideast',\n",
    "    #  'talk.politics.misc',\n",
    "    #  'talk.religion.misc'\n",
    "    ]\n",
    "\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / '20news-bydate.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        original_data = fetch_20newsgroups(\n",
    "            data_home=str(data_home),\n",
    "            subset='all',\n",
    "            categories=categories,\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            load_archive=str(file_path)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        original_data = fetch_20newsgroups(\n",
    "            data_home=data_home,\n",
    "            subset='all',\n",
    "            categories=categories,\n",
    "            remove=('headers','footers','quotes'),\n",
    "            download_if_missing=True\n",
    "        )\n",
    "\n",
    "    # Make tfidf dataset\n",
    "    n_words=200\n",
    "    vectorizer = CountVectorizer(\n",
    "    max_features=n_words, \n",
    "    binary=True,\n",
    "    analyzer=\"word\", \n",
    "    stop_words=\"english\",\n",
    "    strip_accents =\"ascii\",\n",
    "    token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b' #This token ignores words with numbers and requires words to have lenght>=2\n",
    "    )\n",
    "\n",
    "    vectors = vectorizer.fit_transform(original_data.data)\n",
    "    instances = vectors.toarray()\n",
    "    classes = np.asarray(list(map(lambda name: 1 if pos in name else 0 if neg in name else np.NaN, original_data.filenames))).reshape(-1,1)\n",
    "    # print(instances.shape, classes.shape)\n",
    "\n",
    "    data = np.concatenate([instances,classes], axis=1)\n",
    "    # scikit-learn >0.20.0 get_feature_names-> get_feature_names_out \n",
    "    df = pd.DataFrame(data, columns=(np.append(vectorizer.get_feature_names_out(),[\"class\"]))).dropna()\n",
    "    \n",
    "    # N P distribution\n",
    "    df[\"class\"].value_counts()\n",
    "\n",
    "    # normalize\n",
    "    for column in df.columns.values:\n",
    "        df[column]=pd.to_numeric(df[column])\n",
    "    \n",
    "    normalized_df=(df.astype(float)-df.min())/(df.max()-df.min())*2-1\n",
    "    normalized_df[\"class\"] = df[\"class\"]\n",
    "    df = normalized_df\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "\n",
    "    # move class to back\n",
    "\n",
    "    cols = list(df.columns.values) # Make a list of all of the columns in the df\n",
    "    cols.pop(cols.index('class')) # Remove class from list\n",
    "    df = df[cols+['class']]\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "    # 分离 features 和 targets\n",
    "    X_cleaned = df.iloc[:, :-1]\n",
    "    y_cleaned = df.iloc[:, -1]\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    features = torch.Tensor(X_cleaned.to_numpy())\n",
    "    targets = torch.Tensor(y_cleaned.to_numpy()).flatten()\n",
    "    print(f'{features.shape}, p:{int(torch.sum(targets).item())}, n:{torch.sum(targets != 1).item()}')\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "def get_minst_dataset():\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / 'mnist_784.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        return load('mnist_784.pkz')\n",
    "    \n",
    "    x, y = fetch_openml(\"mnist_784\",\n",
    "                        version=1,\n",
    "                        return_X_y=True,\n",
    "                        parser='auto',\n",
    "                        as_frame=False)\n",
    "\n",
    "    features = np.reshape(x, (x.shape[0], 1, 28, 28)) / 255.\n",
    "    # convert_to_binary_label\n",
    "    targets = np.where(np.isin(y, ['0', '2', '4', '6', '8']), 0, 1)\n",
    "\n",
    "    dump((features,targets), 'mnist_784.pkz')\n",
    "    \n",
    "    return features,targets\n",
    "\n",
    "def get_cifar_10():\n",
    "    \"\"\"\n",
    "    CIFAR-10, the positive dataset is formed by ‘airplane’,\n",
    "    ‘automobile’, ‘ship’ and ‘truck’, and the negative dataset is formed by ‘bird’, ‘cat’, ‘deer’, ‘dog’,\n",
    "    ‘frog’ and ‘horse’.\n",
    "    \"\"\"\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / 'CIFAR_10.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        return load('CIFAR_10.pkz')\n",
    "    \n",
    "    x, y = fetch_openml(\"CIFAR_10\",\n",
    "                        version=1,\n",
    "                        return_X_y=True,\n",
    "                        parser='auto',\n",
    "                        as_frame=False)\n",
    "    features = np.reshape(x , (np.shape(x)[0], 3, 32, 32)).astype(np.float32)\n",
    "    targets = np.where(np.isin(y, ['2', '3', '4', '5', '6','7']), 0, 1)\n",
    "\n",
    "    \n",
    "    dump((features,targets), 'CIFAR_10.pkz')\n",
    "    return features, targets\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "53af2577-52bb-483e-955d-b3df2897f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8870, 200]), p:4891, n:3979\n"
     ]
    }
   ],
   "source": [
    "# get_cifar_10()\n",
    "# get_news_dataset_by_categories()\n",
    "# get_uci_dataset_by_id()\n",
    "features,targets = get_news_dataset_by_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "41580db2-7bc9-41df-857d-9d01e5ce2a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1.,  1., -1.,  ..., -1., -1., -1.]]),\n",
       " tensor([0., 0., 0.,  ..., 1., 0., 1.]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1abf8-e0d2-480d-abad-316af1370a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90497582-9ddf-4d7d-8f4d-90e90eee6aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b69baf-78cb-47d1-a01c-cac824406dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
