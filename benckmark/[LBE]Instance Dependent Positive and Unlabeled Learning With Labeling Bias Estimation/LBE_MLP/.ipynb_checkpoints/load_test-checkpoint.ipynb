{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97155842-4150-4241-933f-3299c7efb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import torch as t\n",
    "def reader_breast():\n",
    "    data = scio.loadmat('datasets/breast.mat')['breast'].toarray()\n",
    "    features, targets = t.Tensor(data[:, :-1]), t.Tensor(data[:, -1])\n",
    "    targets = t.where(targets != 1, t.zeros_like(targets), targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cb136-4b19-473a-bcde-f77a0a18abe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87df8680-313f-4405-b990-90433f3c4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_australian():\n",
    "    data = scio.loadmat('datasets/Australian.mat')\n",
    "    features, targets = data['fea'], data['gnd']\n",
    "    features, targets = t.Tensor(features.toarray()), t.Tensor(targets).squeeze()\n",
    "    targets = t.where(targets != 1, t.zeros_like(targets), targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26024721-2058-4543-b86c-a7a3053ee626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87790d74-6ead-4303-a4b1-23c3712b9f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "03deb6d7-c279-4cba-9198-2e896fca2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from joblib import dump,load\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.datasets import fetch_20newsgroups,fetch_openml\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "os.environ['http_proxy'] = '127.0.0.1:1066'\n",
    "os.environ['https_proxy'] = '127.0.0.1:1066'\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def _preprocess_data(uci_id):\n",
    "        # fetch dataset\n",
    "        dataset = fetch_ucirepo(id=uci_id)\n",
    "        \n",
    "        # data (as pandas dataframes)\n",
    "        X = dataset.data.features\n",
    "        y = dataset.data.targets\n",
    "        \n",
    "        # 检测并删除缺失值\n",
    "        combined = pd.concat([X, y], axis=1)\n",
    "        combined_cleaned = combined.dropna()\n",
    "\n",
    "        # 分离 features 和 targets\n",
    "        X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "        y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "        # 转换为 PyTorch 张量\n",
    "        features = torch.Tensor(X_cleaned.to_numpy())\n",
    "        targets = torch.Tensor(y_cleaned.to_numpy()).flatten()\n",
    "\n",
    "        return features, targets\n",
    "\n",
    "def get_uci_dataset_by_id(uci_id):\n",
    "\n",
    "    match int(uci_id):\n",
    "        case 15:\n",
    "            # Breast\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 4, torch.tensor(0.), torch.tensor(1.)) # 4为恶性 2为良性 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 143:\n",
    "            # Australian\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 267:\n",
    "            # banknote\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 171:\n",
    "            # Madelon\n",
    "            pass\n",
    "        case 327:\n",
    "            # Phishing\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        case 105:\n",
    "            # Vote\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 1 为正类 2为负 改为1,0\n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "        # case 73:\n",
    "        #     # fetch dataset\n",
    "        #     dataset = fetch_ucirepo(id=uci_id)\n",
    "            \n",
    "        #     # data (as pandas dataframes)\n",
    "        #     X = dataset.data.features\n",
    "        #     y = dataset.data.targets\n",
    "        #     # 检测并删除缺失值\n",
    "        #     combined = pd.concat([X, y], axis=1)\n",
    "        #     combined_cleaned = combined.dropna()\n",
    "    \n",
    "        #     # 分离 features 和 targets\n",
    "        #     X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "        #     y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "        #     # print(X_cleaned)\n",
    "        #     from sklearn.preprocessing import LabelEncoder\n",
    "        #     label_encoder_X = LabelEncoder()\n",
    "        #     X_cleaned = X_cleaned.apply(lambda col: label_encoder_X.fit_transform(col))\n",
    "            \n",
    "        #     label_encoder_y = LabelEncoder()\n",
    "        #     y_cleaned = label_encoder_y.fit_transform(y_cleaned)\n",
    "    \n",
    "        #     # 转换为 PyTorch 张量\n",
    "        #     features = torch.Tensor(X_cleaned.to_numpy())\n",
    "        #     targets = torch.Tensor(y_cleaned).flatten()\n",
    "        #     return features, targets\n",
    "\n",
    "        case 73:\n",
    "            # fetch dataset\n",
    "            dataset = fetch_ucirepo(id=uci_id)\n",
    "            \n",
    "            # data (as pandas dataframes)\n",
    "            X = dataset.data.features\n",
    "            y = dataset.data.targets\n",
    "            # 检测并删除缺失值\n",
    "            combined = pd.concat([X, y], axis=1)\n",
    "            combined_cleaned = combined.dropna()\n",
    "    \n",
    "            # 分离 features 和 targets\n",
    "            X_cleaned = combined_cleaned.iloc[:, :-1]\n",
    "            y_cleaned = combined_cleaned.iloc[:, -1]\n",
    "\n",
    "            X_vectorized = pd.get_dummies(X_cleaned)\n",
    "            \n",
    "            # 如果 y 是分类变量，也需要转换\n",
    "            # 比如如果 y 也是 'p' 和 'e' 两种类别\n",
    "            y_vectorized = pd.get_dummies(y_cleaned)\n",
    "    \n",
    "            # 转换为 PyTorch 张量\n",
    "            features = torch.Tensor(X_vectorized.to_numpy())\n",
    "            targets = torch.Tensor(y_vectorized.to_numpy())[:,1]\n",
    "            return features, targets\n",
    "        \n",
    "    \n",
    "        case _:\n",
    "            # 所有不需要特殊处理的都可以直接转换\n",
    "            features, targets = _preprocess_data(uci_id)\n",
    "            targets = torch.where(targets != 1, torch.tensor(0.), torch.tensor(1.)) # 改为1,0 \n",
    "            print(f'{features.shape}, p:{torch.sum(targets).item()}, n:{torch.sum(targets != 1).item()}')\n",
    "            return features, targets\n",
    "\n",
    "def get_news_dataset_by_categories():\n",
    "    \n",
    "    pos=\"comp\"\n",
    "    neg=\"rec\"\n",
    "    categories = [\n",
    "    #  'alt.atheism',\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "    #  'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "    #  'sci.crypt',\n",
    "    #  'sci.electronics',\n",
    "    #  'sci.med',\n",
    "    #  'sci.space',\n",
    "    #  'soc.religion.christian',\n",
    "    #  'talk.politics.guns',\n",
    "    #  'talk.politics.mideast',\n",
    "    #  'talk.politics.misc',\n",
    "    #  'talk.religion.misc'\n",
    "    ]\n",
    "\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / '20news-bydate.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        original_data = fetch_20newsgroups(\n",
    "            data_home=str(data_home),\n",
    "            subset='all',\n",
    "            categories=categories,\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            load_archive=str(file_path)\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        original_data = fetch_20newsgroups(\n",
    "            data_home=data_home,\n",
    "            subset='all',\n",
    "            categories=categories,\n",
    "            remove=('headers','footers','quotes'),\n",
    "            download_if_missing=True\n",
    "        )\n",
    "\n",
    "    # Make tfidf dataset\n",
    "    n_words=200\n",
    "    vectorizer = CountVectorizer(\n",
    "    max_features=n_words, \n",
    "    binary=True,\n",
    "    analyzer=\"word\", \n",
    "    stop_words=\"english\",\n",
    "    strip_accents =\"ascii\",\n",
    "    token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b' #This token ignores words with numbers and requires words to have lenght>=2\n",
    "    )\n",
    "\n",
    "    vectors = vectorizer.fit_transform(original_data.data)\n",
    "    instances = vectors.toarray()\n",
    "    classes = np.asarray(list(map(lambda name: 1 if pos in name else 0 if neg in name else np.NaN, original_data.filenames))).reshape(-1,1)\n",
    "    # print(instances.shape, classes.shape)\n",
    "\n",
    "    data = np.concatenate([instances,classes], axis=1)\n",
    "    # scikit-learn >0.20.0 get_feature_names-> get_feature_names_out \n",
    "    df = pd.DataFrame(data, columns=(np.append(vectorizer.get_feature_names_out(),[\"class\"]))).dropna()\n",
    "    \n",
    "    # N P distribution\n",
    "    df[\"class\"].value_counts()\n",
    "\n",
    "    # normalize\n",
    "    for column in df.columns.values:\n",
    "        df[column]=pd.to_numeric(df[column])\n",
    "    \n",
    "    normalized_df=(df.astype(float)-df.min())/(df.max()-df.min())*2-1\n",
    "    normalized_df[\"class\"] = df[\"class\"]\n",
    "    df = normalized_df\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "\n",
    "    # move class to back\n",
    "\n",
    "    cols = list(df.columns.values) # Make a list of all of the columns in the df\n",
    "    cols.pop(cols.index('class')) # Remove class from list\n",
    "    df = df[cols+['class']]\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "    # 分离 features 和 targets\n",
    "    X_cleaned = df.iloc[:, :-1]\n",
    "    y_cleaned = df.iloc[:, -1]\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    features = torch.Tensor(X_cleaned.to_numpy())\n",
    "    targets = torch.Tensor(y_cleaned.to_numpy()).flatten()\n",
    "    print(f'{features.shape}, p:{int(torch.sum(targets).item())}, n:{torch.sum(targets != 1).item()}')\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "def get_minst_dataset():\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / 'mnist_784.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        return load('mnist_784.pkz')\n",
    "    \n",
    "    x, y = fetch_openml(\"mnist_784\",\n",
    "                        version=1,\n",
    "                        return_X_y=True,\n",
    "                        parser='auto',\n",
    "                        as_frame=False)\n",
    "\n",
    "    features = np.reshape(x, (x.shape[0], 1, 28, 28)) / 255.\n",
    "    # convert_to_binary_label\n",
    "    targets = np.where(np.isin(y, ['0', '2', '4', '6', '8']), 0, 1)\n",
    "\n",
    "    dump((features,targets), 'mnist_784.pkz')\n",
    "    \n",
    "    return features,targets\n",
    "\n",
    "def get_cifar_10():\n",
    "    \"\"\"\n",
    "    CIFAR-10, the positive dataset is formed by ‘airplane’,\n",
    "    ‘automobile’, ‘ship’ and ‘truck’, and the negative dataset is formed by ‘bird’, ‘cat’, ‘deer’, ‘dog’,\n",
    "    ‘frog’ and ‘horse’.\n",
    "    \"\"\"\n",
    "    data_home = Path('./')\n",
    "    file_path = data_home / 'CIFAR_10.pkz'\n",
    "\n",
    "    if file_path.exists():\n",
    "        return load('CIFAR_10.pkz')\n",
    "    \n",
    "    x, y = fetch_openml(\"CIFAR_10\",\n",
    "                        version=1,\n",
    "                        return_X_y=True,\n",
    "                        parser='auto',\n",
    "                        as_frame=False)\n",
    "    features = np.reshape(x , (np.shape(x)[0], 3, 32, 32)).astype(np.float32)\n",
    "    targets = np.where(np.isin(y, ['2', '3', '4', '5', '6','7']), 0, 1)\n",
    "\n",
    "    \n",
    "    dump((features,targets), 'CIFAR_10.pkz')\n",
    "    return features, targets\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "53af2577-52bb-483e-955d-b3df2897f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_cifar_10()\n",
    "# get_news_dataset_by_categories()\n",
    "# get_uci_dataset_by_id()\n",
    "features,targets = get_minst_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "41580db2-7bc9-41df-857d-9d01e5ce2a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[ 26.,  17.,  13., ...,  15.,  24.,  22.],\n",
       "          [ 20.,  13.,  13., ...,  19.,  21.,  29.],\n",
       "          [ 14.,  13.,  13., ...,  17.,  25.,  31.],\n",
       "          ...,\n",
       "          [ 90.,  34.,  28., ...,  23.,  16.,   9.],\n",
       "          [ 79.,  58.,  32., ...,  14.,  16.,  10.],\n",
       "          [128.,  58.,  25., ...,  13.,  12.,  13.]],\n",
       " \n",
       "         [[ 23.,  14.,   9., ...,  14.,  24.,  21.],\n",
       "          [ 17.,  10.,   9., ...,  17.,  20.,  29.],\n",
       "          [ 11.,  10.,   9., ...,  16.,  24.,  31.],\n",
       "          ...,\n",
       "          [109.,  64.,  54., ...,  20.,  13.,   6.],\n",
       "          [105.,  96.,  68., ...,  11.,  13.,   7.],\n",
       "          [157.,  93.,  60., ...,  10.,   9.,  10.]],\n",
       " \n",
       "         [[ 32.,  25.,  24., ...,  28.,  37.,  34.],\n",
       "          [ 26.,  22.,  24., ...,  35.,  35.,  39.],\n",
       "          [ 20.,  21.,  23., ...,  32.,  38.,  42.],\n",
       "          ...,\n",
       "          [137.,  95.,  90., ...,  37.,  30.,  23.],\n",
       "          [141., 139., 110., ...,  28.,  30.,  24.],\n",
       "          [196., 149., 106., ...,  27.,  26.,  27.]]],\n",
       " \n",
       " \n",
       "        [[[ 94., 101.,  95., ..., 145., 145., 121.],\n",
       "          [ 89.,  97., 101., ..., 146., 146., 122.],\n",
       "          [ 86.,  94., 107., ..., 146., 147., 123.],\n",
       "          ...,\n",
       "          [205., 208., 201., ..., 145., 149., 125.],\n",
       "          [201., 205., 198., ..., 154., 158., 134.],\n",
       "          [190., 188., 175., ..., 162., 163., 135.]],\n",
       " \n",
       "         [[ 86.,  91.,  85., ..., 138., 140., 117.],\n",
       "          [ 84.,  89.,  91., ..., 137., 139., 117.],\n",
       "          [ 84.,  88.,  98., ..., 136., 138., 116.],\n",
       "          ...,\n",
       "          [205., 208., 201., ..., 149., 152., 128.],\n",
       "          [201., 205., 198., ..., 158., 162., 138.],\n",
       "          [190., 188., 175., ..., 164., 166., 137.]],\n",
       " \n",
       "         [[ 58.,  61.,  54., ..., 106., 108.,  90.],\n",
       "          [ 55.,  59.,  60., ..., 106., 107.,  90.],\n",
       "          [ 54.,  57.,  67., ..., 106., 107.,  90.],\n",
       "          ...,\n",
       "          [204., 207., 200., ..., 149., 157., 135.],\n",
       "          [201., 205., 198., ..., 169., 177., 153.],\n",
       "          [189., 187., 173., ..., 182., 184., 155.]]],\n",
       " \n",
       " \n",
       "        [[[183., 158., 166., ..., 142., 137., 144.],\n",
       "          [125.,  97., 100., ...,  63.,  59.,  67.],\n",
       "          [107.,  85.,  88., ...,  63.,  53.,  60.],\n",
       "          ...,\n",
       "          [108.,  98., 119., ..., 114., 104.,  91.],\n",
       "          [199., 197., 207., ..., 203., 200., 189.],\n",
       "          [251., 248., 249., ..., 250., 250., 250.]],\n",
       " \n",
       "         [[186., 167., 172., ..., 147., 146., 150.],\n",
       "          [134., 111., 112., ...,  74.,  74.,  79.],\n",
       "          [121., 104., 104., ...,  79.,  73.,  77.],\n",
       "          ...,\n",
       "          [116., 107., 130., ..., 125., 115., 101.],\n",
       "          [205., 203., 214., ..., 209., 205., 194.],\n",
       "          [255., 252., 253., ..., 251., 250., 251.]],\n",
       " \n",
       "         [[178., 152., 160., ..., 144., 141., 146.],\n",
       "          [120.,  90.,  95., ...,  68.,  67.,  72.],\n",
       "          [103.,  79.,  83., ...,  71.,  64.,  69.],\n",
       "          ...,\n",
       "          [103.,  87., 105., ..., 113., 103.,  90.],\n",
       "          [197., 190., 196., ..., 202., 199., 188.],\n",
       "          [252., 246., 244., ..., 250., 250., 250.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[ 20.,  19.,  15., ...,  10.,  12.,  13.],\n",
       "          [ 21.,  20.,  18., ...,  10.,  10.,  12.],\n",
       "          [ 21.,  21.,  20., ...,  12.,  12.,  13.],\n",
       "          ...,\n",
       "          [ 33.,  34.,  34., ...,  28.,  29.,  23.],\n",
       "          [ 33.,  34.,  34., ...,  27.,  27.,  25.],\n",
       "          [ 31.,  32.,  33., ...,  24.,  26.,  25.]],\n",
       " \n",
       "         [[ 15.,  14.,  14., ...,   9.,  11.,  12.],\n",
       "          [ 16.,  16.,  17., ...,   9.,   9.,  11.],\n",
       "          [ 16.,  17.,  18., ...,  11.,  11.,  12.],\n",
       "          ...,\n",
       "          [ 25.,  26.,  26., ...,  25.,  25.,  20.],\n",
       "          [ 25.,  26.,  26., ...,  24.,  24.,  22.],\n",
       "          [ 23.,  24.,  25., ...,  23.,  23.,  20.]],\n",
       " \n",
       "         [[ 12.,  11.,  11., ...,   7.,   9.,  10.],\n",
       "          [ 13.,  13.,  12., ...,   7.,   7.,   9.],\n",
       "          [ 13.,  12.,  11., ...,   9.,   9.,  10.],\n",
       "          ...,\n",
       "          [ 13.,  15.,  15., ...,  52.,  58.,  42.],\n",
       "          [ 14.,  15.,  15., ...,  52.,  56.,  47.],\n",
       "          [ 12.,  13.,  14., ...,  50.,  53.,  47.]]],\n",
       " \n",
       " \n",
       "        [[[ 25.,  15.,  23., ...,  61.,  92.,  75.],\n",
       "          [ 12.,  20.,  24., ..., 115., 149., 104.],\n",
       "          [ 12.,  15.,  34., ..., 154., 157., 116.],\n",
       "          ...,\n",
       "          [100., 103., 104., ...,  97.,  98.,  91.],\n",
       "          [103., 104., 107., ..., 101.,  99.,  92.],\n",
       "          [ 95.,  95., 101., ...,  93.,  95.,  92.]],\n",
       " \n",
       "         [[ 40.,  36.,  41., ...,  82., 113.,  89.],\n",
       "          [ 25.,  37.,  36., ..., 134., 168., 117.],\n",
       "          [ 25.,  29.,  40., ..., 172., 175., 129.],\n",
       "          ...,\n",
       "          [129., 132., 134., ..., 128., 126., 121.],\n",
       "          [132., 131., 135., ..., 132., 127., 121.],\n",
       "          [126., 123., 128., ..., 124., 123., 120.]],\n",
       " \n",
       "         [[ 12.,   3.,  18., ...,  78., 112.,  92.],\n",
       "          [  6.,   7.,  15., ..., 138., 177., 131.],\n",
       "          [ 11.,   6.,  24., ..., 182., 192., 151.],\n",
       "          ...,\n",
       "          [ 81.,  84.,  86., ...,  84.,  84.,  79.],\n",
       "          [ 83.,  83.,  87., ...,  87.,  84.,  79.],\n",
       "          [ 78.,  76.,  81., ...,  80.,  81.,  80.]]],\n",
       " \n",
       " \n",
       "        [[[ 73.,  98.,  99., ..., 135., 135., 203.],\n",
       "          [ 69.,  84.,  68., ...,  85.,  71., 120.],\n",
       "          [ 69.,  90.,  62., ...,  74.,  53.,  62.],\n",
       "          ...,\n",
       "          [123., 132., 129., ..., 108.,  62.,  27.],\n",
       "          [115., 123., 129., ..., 115.,  66.,  27.],\n",
       "          [116., 121., 129., ..., 116.,  68.,  27.]],\n",
       " \n",
       "         [[ 78., 103., 106., ..., 150., 149., 215.],\n",
       "          [ 73.,  89.,  75., ...,  95.,  82., 133.],\n",
       "          [ 73.,  95.,  71., ...,  81.,  62.,  74.],\n",
       "          ...,\n",
       "          [128., 132., 128., ..., 107.,  60.,  27.],\n",
       "          [121., 124., 126., ..., 116.,  65.,  27.],\n",
       "          [120., 122., 128., ..., 115.,  65.,  26.]],\n",
       " \n",
       "         [[ 75., 113., 114., ..., 152., 154., 223.],\n",
       "          [ 70.,  97.,  81., ...,  89.,  80., 135.],\n",
       "          [ 70., 100.,  74., ...,  70.,  54.,  69.],\n",
       "          ...,\n",
       "          [ 96., 102., 100., ...,  88.,  55.,  28.],\n",
       "          [ 91.,  95.,  99., ...,  94.,  59.,  27.],\n",
       "          [ 90.,  94., 101., ...,  94.,  58.,  26.]]]], dtype=float32),\n",
       " array([1, 0, 1, ..., 0, 1, 0]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1abf8-e0d2-480d-abad-316af1370a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90497582-9ddf-4d7d-8f4d-90e90eee6aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b69baf-78cb-47d1-a01c-cac824406dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
